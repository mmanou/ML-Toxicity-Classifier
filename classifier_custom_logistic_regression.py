import math
from typing import Optional
from time import perf_counter
import lib_processing


TRAIN_MAX_INSTANCES: Optional[int] = None
TEST_MAX_INSTANCES: Optional[int] = None

RUN_TRAINING = True
NUM_EPOCHS = 100
THRESHOLD = 0.5
BIAS = 0.0
LEARN_RATE = 0.001


def main() -> None:
    t0 = perf_counter()
    print("Loading data")
    (train_instances, train_actual_labels, train_feature_column_names)\
        = lib_processing.preprocess_labelled_data("dataset/train_embedding.csv", TRAIN_MAX_INSTANCES)

    (test_instances, test_actual_labels, test_feature_column_names)\
        = lib_processing.preprocess_labelled_data("dataset/dev_embedding.csv", TEST_MAX_INSTANCES)

    if len(train_instances[0]) != len(train_feature_column_names):
        raise Exception("Mismatched vector lengths: instances[0], feature_column_names")
    if train_feature_column_names != test_feature_column_names:
        raise Exception("train-test column names do not match")

    train_instances = lib_processing.remove_identity_columns(train_instances)
    test_instances = lib_processing.remove_identity_columns(test_instances)

    theta_vector = [0.001 for _ in range(len(train_instances[0]))]
    # theta_vector = [0.3, -2.2, 3.3, -0.2]
    # theta_vector = [-1.5039262845439423, -0.6997860900680025, -0.19846833778920456, -1.2098218403662737, -0.35674101962486426, -8.690351418476146, -16.087733862228518, -0.08196260872893213, -0.5421075492754223, -0.2578515272886439, -0.19566986260043043, -1.9113834139040018, -0.7291398237885145, -10.116909169053056, -5.96400378673442, 0.00561784220532575, 0.025998404065207464, -0.3268293747313682, -0.08604847491937453, 0.0008989790316293886, -0.10825716127891892, -0.7001471038824846, 0.22761654167333983, -5.2473125031724965, -0.8428739070813964, -2.09041413804632, -0.37472975698577987, -0.22358594301821239, -1.3131053409278428, -1.7653042454149661, 1.0430150604741502, 2.0257522650671276, 0.27208960090744927, 0.19284095869767473, -0.4255668603754573, -2.821339294304941, 0.13572397787392781, 0.5153261377992895, -2.3331210259342443, -1.1516248624931151, 0.0014028744539586468, 0.10287547441249151, 0.9609853402188727, -2.0749324836157648, 2.7249636630606977, 0.8252830974610157, -1.0197005851501844, -0.567286571570889, 0.020227250431127164, 0.9585701317538522, -0.34623912090991743, 1.3005206826000308, 0.20669803334542053, 0.13806586413244704, -0.32053976606322454, -0.74696538722194, -0.6558341965610748, -0.46544973913608223, 0.9034956340959954, -1.558992470883875, -1.2422834059031675, -0.5360801073744318, -0.4867805357630559, 0.0636963517253048, -0.7783590151287253, 1.0954273042481337, 0.18529837564834245, 0.7990195481054838, 0.6425733573040998, 0.8210221434376151, -0.41118944080596914, 0.8515771571916912, 3.2952894627419598, 0.6988996767098667, 0.1686526429378758, 0.26284378901719163, -0.5984047907013434, 2.0753026243919623, 0.14228990534137753, 0.019025391570199224, 0.9179798655281919, 0.21732676681211088, -1.0224655119626969, 0.7023719664321393, 0.31503946516810855, 0.4935482987589398, 0.9271235609470525, -1.4245472167990232, -0.33454328851999776, 0.9876982563413788, 0.7383566100412147, 1.1716301044219077, 0.6793498783927083, -0.3308126932749822, -1.808487577787227, -1.7660895977981932, -1.023195757045547, 0.132861137128084, 0.9052867271617258, 1.943677731291973, -1.2665792727553011, -1.2105356039344326, -0.7432070560862466, 2.1448132139547353, -1.807341509369707, 1.109173379756508, -0.19004523864464412, -1.7226078731677081, 0.5011338049457438, 1.0840335943882318, 1.046225278633164, 2.83561440709067, 0.024866921658885627, -0.9738416982588838, 2.3787296590425453, 0.2949165592285241, -2.02175093150914, 0.5928990815142415, -1.3199151339498643, -0.44398532777895194, 1.7602824384551943, -0.9721326331909624, 1.787825657426702, -0.7228647721836834, 1.0946662700090288, -2.293681179661644, -0.16382917038174322, 1.6323208432786436, 1.357435295070155, 2.7622643858328737, 1.4378119995024927, 1.2243527549856417, 2.017626630336203, -0.9901126172420767, -0.8001200622985675, 0.8380627431199865, 1.501320456730074, -0.17046758953082541, -1.2871205318198262, 0.8689270813619089, -0.9209032256437397, 0.838553174528267, 0.22284096626936467, -0.11962224984137003, 1.029592974987063, -0.4444400402905011, 0.48916327577769736, 0.19551950477544966, -0.24583990667447647, -0.900757808081798, 0.22377027012919826, 0.001, 0.6844797961474451, 0.9228091523912991, -0.6639099758497167, -1.08846588792104, -0.09784281264040734, -0.6528261301526315, 1.1225194993814802, 1.2123957967801964, 0.6940221250369543, 1.0473593291640915, -1.0509203592566903, -0.785805413830661, -2.6365369683030044, 1.515573885928827, -0.1817426742336894, 0.561918919166676, 3.082585141247458, 0.8970710718872312, 0.045456499900636105, -0.7953634062993631, -2.3260232691207543, -1.2465030260906658, 1.401403472746207, -0.04775720115206983, 1.10599863352375, 1.6773830754531645, -1.4401352376946792, -1.3399214110949589, -0.2256629961739594, -0.24034667218789343, 2.711231082368845, -1.2626575552919401, -0.8045648512683607, 1.1081010498927122, -1.7187979667686655, -1.1905414170423936, 0.33103806282653825, 0.14953572772221338, 0.19941873692806722, 0.5426746198154503, 1.676983559758936, -0.45015298674981896, -2.0248813959235292, -0.11737363436504597, -0.3364561065334364, -0.3729902756569011, -1.1065595466561136, 1.779334413294891, 2.5244862129286885, -1.552135058549958, -0.06858999821813638, -1.250090130987059, 0.10982266210231983, 1.1252013155675498, 1.0282908213465825, 0.7431280998852251, 1.6771606427378614, -2.331560741639138, -0.8892916751587052, 0.635684700841302, -0.5240929453180054, 0.8675266950226732, 0.3206147688002621, -0.253794432621698, 1.2742032494380755, -1.8583842942061888, 0.3911102835648127, -0.01612494706996523, 0.6764324898405686, -0.26696233885180065, -0.565056784470717, -0.37036965900425506, 1.3282833616114251, -0.7274490688881877, -0.1564699813122643, -1.7738864177135876, 0.6429484578730004, -1.5068083401059007, -2.2365752113072674, 1.7302861399648295, 0.5253563313796946, 0.8026059976497353, -1.8497797044726483, -2.0679585879062774, -1.496509718070112, 1.2587129502841181, -1.6804537345707975, 0.34603654048177956, 0.4967925227246366, -1.760459861235459, -2.1946727304738407, -0.7259975609452975, -1.2039378531926013, -0.04707476216184234, 1.1472792230840885, 0.001, 0.32595850094167406, 0.6211655224099257, 0.07313470793702997, 0.3984825086164582, -0.81402769528084, 0.9986529783656524, 1.0193021440079355, 0.9800292563712424, -1.5581944495288842, 0.521151468159565, -1.1979339009737302, 0.6407764519558172, -1.038477147086187, -1.9954664499383652, 3.644872160332504, 1.744985685825494, 0.45028570750767394, -0.3483654706800957, -0.34523371113594603, 1.0805516785428246, -1.1783481541186562, -0.45189618588322267, 1.152759236813627, -0.9211425541038961, -0.45822006066784027, -1.302802261843809, 0.27153413983015284, 2.2655075231940693, 0.9024752538335272, 1.5335131660819599, 1.0418325460565554, -0.48401803125073173, 1.175110782259992, 0.3475976996174638, -0.18348151414353414, -0.937070116467069, -0.18649561627211764, -1.014992703629504, -0.027625026250128665, -0.5406644190596888, -0.4757500003575552, 0.7822287761169725, 1.730870610890489, -0.7052182897951806, 0.7258071919601445, -1.434757423775564, -1.046751500313159, -1.720121997636506, -0.1655299312998412, 0.16462281663349787, 2.6603776188126567, 0.6088857614361052, 0.462962799893821, -1.3414190350331157, -0.054554891140102674, -0.0022638782310263617, -0.8894109717390998, 1.4058298855750007, 0.09985673255025815, -1.3156238379694332, 0.11127927836193419, -1.7506359887060094, 2.0423668015522463, 0.6875101676754549, -0.0992359497198505, 0.4411714014998119, 0.9186561833718697, 0.8961549432126594, -1.0763043278534972, -0.6052926576616333, -0.8681898052554227, 3.6602739150035313, 2.216350924401307, -0.005942405962011288, -1.258892847157319, -1.208526386856398, -0.27836407525779533, 1.0156595964706643, 2.075291400568238, -0.044192970571395235, -0.7610777778854971, 3.3665816982906485, 1.1451206429639933, 0.6990520961441314, -1.276881682721564, -0.6236363848372793, 0.7723279527015807, 2.304608285828934, 0.18500673427665756, -1.654614512386132, 1.6332497008010582, 0.783924420639729, 0.590565002172446, 1.727661214055271, 0.1703235115248, 0.0010041318388583835, -1.0919375675400136, -1.46958558008782, 0.4320137310141077, -0.29067056062840235, -0.3457655006033571, 0.5148640566452443, -1.5472417266182283, 3.6221398674208967, 1.8433467214834964, -3.3611972289791483, 0.9958244286725373, -1.6849102023690383, 0.328122995569162, 0.4369078496591896, -0.9263749385681277, 0.7446405290594531, -1.4979018053108901, 1.997444087323681, 1.0751743626225907, -0.42872529220131683, -1.3430665158088033, 0.871685125212668, 0.695497786719193, 0.8804797615123242, -0.16116996744727785, -0.9010496255508919, 1.677186372771613, -0.3379897622628361, 1.6980867353263716, -0.20714264578181563, 0.2443624262627125, -0.8953656926444782, 2.7255523001081654, 0.6099279280888052, -0.06985694093935646, 0.10363759620022935, 1.0486567986000739, -0.3549266597372891, -1.9204272756132013, 2.0527898334232977, -0.48948752887447927, 0.13605464983814458, -0.9243212576938302, -2.403626008958332, -1.090493465402294, 0.3040858469686756, -0.07503399257575782, -2.529229840092164, 0.6013132036368768, -0.9613076361124548, -0.5184897831258035, 0.04160971345542819, -1.7373258554216273, 0.3381869710762209, -1.2666840028056199, -1.5181991772203867, 0.17181311420463283, -0.7860880236510004, 1.0913759704572978, 1.3853008847993245, -1.7417236926396904, 2.214535089076799, 0.845088028721602, 0.36955422061358767]
    theta_vector.insert(0, BIAS)

    if RUN_TRAINING:
        print("Training logistic regression model")
        theta_vector = train(train_instances, train_actual_labels, theta_vector, LEARN_RATE, NUM_EPOCHS, THRESHOLD)

    print("final theta_vector:")
    print(theta_vector)

    t1 = perf_counter()
    print(f"\nTraining time: {t1 - t0}\n")

    print("performance against training set:")
    (predictions, _sigmas) = predict(train_instances, theta_vector, THRESHOLD)

    lib_processing.eval_labels(predictions, train_actual_labels)

    print("performance against test set:")
    (predictions, _sigmas) = predict(test_instances, theta_vector, THRESHOLD)

    lib_processing.eval_labels(predictions, test_actual_labels)

    t2 = perf_counter()
    print(f"Prediction time: {t2 - t1}")


def predict_label_and_sigma(x_vector: list[int, float],
                            theta_vector: list[float],
                            threshold: float
                            ) -> (int,
                                  float):
    if (len(x_vector) + 1) != len(theta_vector):
        raise Exception("Mismatched vector lengths: x_vector+1, theta_vector")

    theta_transpose_x = theta_vector[0]  # initialise with bias
    theta_vector = theta_vector[1:]  # remove bias from vector

    theta_transpose_x += sum(x * theta for x, theta in zip(x_vector, theta_vector))
    # theta_transpose_x = sum(x_vector[i] * theta_vector[i] for i in range(len(x_vector)))
    # for i in range(len(x_vector)):
    #     theta_transpose_x += x_vector[i] * theta_vector[i]

    denom = 1.0 + math.exp(-theta_transpose_x)
    sigma = 1.0 / denom                           # 1 / (1 + e^(-ttx) )
    prediction = 1 if (sigma > threshold) else 0
    return prediction, sigma


def predict(instances: list[list[int, float]],
            theta_vector: list[float],
            threshold: float
            ) -> (list[int],
                  list[float]):
    predict_results: list[(int, float)] = [predict_label_and_sigma(x_vector, theta_vector, threshold)
                                           for x_vector in instances]

    return zip(*predict_results)  # list of predictions, list of sigmas


def update_weights(instances: list[list[int, float]],
                   theta_vector: list[float],
                   sigmas: list[float],
                   predicted_labels: list[int],
                   actual_labels: list[int],
                   learn_rate: float
                   ) -> list[float]:
    if (len(sigmas) != len(predicted_labels)
            and len(sigmas) != len(actual_labels)
            and len(sigmas) != len(theta_vector) + 1
            and len(sigmas) != len(instances[0])):
        raise Exception("Mismatched vector lengths: sigmas, predicted_labels, theta_vector+1, instances[0]")

    bias = theta_vector[0]
    theta_vector = theta_vector[1:]
    for j in range(len(theta_vector)):
        theta_vector[j] -= learn_rate * sum((sigmas[i] - actual_labels[i]) * instances[i][j]
                                            for i in range(len(sigmas)))

    theta_vector.insert(0, bias)  # reinsert bias

    return theta_vector


def train(instances: list[list[int, float]],
          actual_labels: list[int],
          theta_vector: list[float],
          learn_rate: float,
          num_epochs: int,
          threshold: float = 0.5
          ) -> list[float]:
    for epoch in range(num_epochs):
        (predicted_labels, sigmas) = predict(instances, theta_vector, threshold)
        theta_vector = update_weights(instances, theta_vector, sigmas, predicted_labels, actual_labels, learn_rate)

        print(f"\nEpoch {str(epoch)}")
        # print("  sigmas:")
        # print(sigmas)
        # print("  predicted_labels:")
        # print(predicted_labels)
        # print("  theta_vector:")
        # print(theta_vector)

    return theta_vector


if __name__ == "__main__":
    main()
